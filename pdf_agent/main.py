# streamlit import
import streamlit as st


# langchain imports
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.vectorstores import InMemoryVectorStore
from langchain_core.output_parsers import StrOutputParser
from langchain_community.document_loaders import PyMuPDFLoader

# ollama imports
from langchain_ollama import OllamaEmbeddings
from base_llm import LOCAL_LLM_INSTANCE as model, EMBEDDING_MODEL


# vector store and embeddings in streamlit session state
if "vector_store" not in st.session_state:
    embeddings = OllamaEmbeddings(model=EMBEDDING_MODEL)
    st.session_state.vector_store = InMemoryVectorStore(embedding=embeddings)


system = """
You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question.
Provide any code block if present in pdf in the answer.
Answer in beautiful "Markdown" format with emojis, bullet points and code blocks whereever applicable.

Assistant ONLY uses information contained in the "Context" section and does not hallucinate.

Context: {documents}

Answer:

"""

prompt = ChatPromptTemplate.from_messages([("system", system), ("human", "{question}")])
chain = prompt | model | StrOutputParser()


# Utility functions
pdfs_directory = "./pdfs/"


def upload_pdf(file):
    with open(pdfs_directory + file.name, "wb") as f:
        f.write(file.getbuffer())


# ==========================# Streamlit App ==========================

st.title("Let Your `PDF` do the talking!!")

# ==========================# Streamlit App ==========================


with st.container(border=True):
    st.info("""
                ##### :information_source: Different Run Results
                :warning: The output generated by AI chat models can vary with each execution due to their dynamic, probabilistic nature. Don't be surprised if your results differ.
                """)

with st.container(border=True):
    st.markdown(
        """
        :information_source: Upload your PDF üìÑ
        - Upload a PDF document to extract text and answer questions based on its content.
        - The AI model will analyze the document and provide relevant information.
        - LLM is hosted locally via ollama, so no need to worry about your data.
        """
    )

uploaded_file = st.file_uploader(
    label="Choose `PDF` File...", type="pdf", accept_multiple_files=False
)

if uploaded_file and st.session_state.get("processed_file") != uploaded_file.name:
    upload_pdf(uploaded_file)

    # with st.status("Please wait while we process your PDF...", expanded=True):
    with st.spinner("Please wait while we process your PDF..."):
        # Load the PDF file using PyMuPDFLoader
        loader = PyMuPDFLoader(pdfs_directory + uploaded_file.name, mode="page")
        documents = loader.load()
        st.toast("PDF uploaded successfully!", icon="‚úÖ")

        # indexing
        st.session_state.vector_store.add_documents(documents[:100])

        st.session_state.processed_file = uploaded_file.name
        st.toast("PDF documents are now indexed into vector store!", icon="üéâ")
        st.success(
            "PDF is now indexed and ready to use, 'Fire your queries'...", icon="‚úÖ"
        )


messages = st.container()

if "processed_file" in st.session_state:
    if question := st.chat_input("Ask me anything from given pdf..."):
        messages.chat_message("user").write(question)

        with st.spinner("Fetching relevent context from pdf..."):
            related_documents = st.session_state.vector_store.search(
                query=question, search_type="mmr"
            )

            documents = "\n\n".join([doc.page_content for doc in related_documents])

            messages.chat_message("assistant").write(
                "Vector store fetch complete. \n\n Now analysing relevent pages for your query. Please wait..."
            )

        with st.spinner("Generating response..."):
            answer = chain.stream({"question": question, "documents": documents})
            messages.chat_message("assistant").write_stream(answer)
else:
    st.warning("Please upload a PDF file to start chatting.", icon="‚ö†Ô∏è")

